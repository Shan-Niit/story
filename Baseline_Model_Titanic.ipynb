{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxAZ+PdyXaIp6nkX2oaxiX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shan-Niit/story/blob/main/Baseline_Model_Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model Titanic"
      ],
      "metadata": {
        "id": "EXPaNA0CusLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the libraries"
      ],
      "metadata": {
        "id": "gUZy-aQIyjGb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-pkRhnNnkzr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the dataset"
      ],
      "metadata": {
        "id": "3gNswZxWypTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Titanic data\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Drop rows where 'Embarked' is missing (only 2 rows)\n",
        "df = df.dropna(subset=['Embarked'])\n"
      ],
      "metadata": {
        "id": "5JDGJ1vux_DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model to predict the missing data for age (+20% of the rows) depending of the other variables"
      ],
      "metadata": {
        "id": "7c-6qa2oyhYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features to help predict Age\n",
        "features_for_age = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "\n",
        "# Create a temporary DataFrame with one-hot encoding\n",
        "df_temp = pd.get_dummies(df[features_for_age + ['Age']], drop_first=True)\n",
        "\n",
        "# Split into known and missing Age\n",
        "df_age_known = df_temp[df_temp['Age'].notnull()]\n",
        "df_age_missing = df_temp[df_temp['Age'].isnull()]\n",
        "\n",
        "# Train the model\n",
        "X_age = df_age_known.drop('Age', axis=1)\n",
        "y_age = df_age_known['Age']\n",
        "\n",
        "rfr = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rfr.fit(X_age, y_age)\n",
        "\n",
        "# Predict missing ages\n",
        "X_missing_age = df_age_missing.drop('Age', axis=1)\n",
        "predicted_ages = rfr.predict(X_missing_age)\n",
        "\n",
        "# Fill back into the original DataFrame\n",
        "df.loc[df['Age'].isnull(), 'Age'] = predicted_ages\n"
      ],
      "metadata": {
        "id": "gl4Ju-W0x-9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing pipeline"
      ],
      "metadata": {
        "id": "iB3tjaRdzFvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = df['Survived']\n",
        "\n",
        "# Column groups\n",
        "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
        "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "\n",
        "# Pipelines\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine pipelines\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_pipeline, numeric_features),\n",
        "    ('cat', categorical_pipeline, categorical_features)\n",
        "])\n",
        "\n",
        "# Apply transformations\n",
        "X_prepro = preprocessor.fit_transform(X)\n",
        "\n",
        "# Get column names\n",
        "num_cols = numeric_features\n",
        "cat_cols = preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical_features)\n",
        "all_columns = list(num_cols) + list(cat_cols)\n",
        "\n",
        "# Wrap in DataFrame\n",
        "X_final = pd.DataFrame(X_prepro, columns=all_columns)\n",
        "\n",
        "# Optional: display the result\n",
        "print(X_final.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvLYx6yxx-6Y",
        "outputId": "3f308b9f-df04-4715-d90a-2080d66c3265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Age     SibSp     Parch      Fare  Pclass_1  Pclass_2  Pclass_3  \\\n",
            "0 -0.545560  0.431350 -0.474326 -0.500240       0.0       0.0       1.0   \n",
            "1  0.619174  0.431350 -0.474326  0.788947       1.0       0.0       0.0   \n",
            "2 -0.254377 -0.475199 -0.474326 -0.486650       0.0       0.0       1.0   \n",
            "3  0.400786  0.431350 -0.474326  0.422861       1.0       0.0       0.0   \n",
            "4  0.400786 -0.475199 -0.474326 -0.484133       0.0       0.0       1.0   \n",
            "\n",
            "   Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
            "0         0.0       1.0         0.0         0.0         1.0  \n",
            "1         1.0       0.0         1.0         0.0         0.0  \n",
            "2         1.0       0.0         0.0         0.0         1.0  \n",
            "3         1.0       0.0         0.0         0.0         1.0  \n",
            "4         0.0       1.0         0.0         0.0         1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import the models"
      ],
      "metadata": {
        "id": "HnWCyQ8xd5fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "pGv-KZ67d5FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define models and their hyperparameters"
      ],
      "metadata": {
        "id": "vR2Hr1myeBuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = {\n",
        "    'SVC': {\n",
        "        'model': SVC(probability=True, random_state=42),\n",
        "        'params': {\n",
        "            'model__C': [0.1, 1, 10],\n",
        "            'model__kernel': ['linear', 'rbf']\n",
        "        }\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'model__n_estimators': [100, 200],\n",
        "            'model__max_depth': [None, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "        'params': {\n",
        "            'model__n_estimators': [100, 200],\n",
        "            'model__max_depth': [3, 5],\n",
        "            'model__learning_rate': [0.05, 0.1]\n",
        "        }\n",
        "    },\n",
        "    'MLP': {\n",
        "        'model': MLPClassifier(max_iter=500, random_state=42),\n",
        "        'params': {\n",
        "            'model__hidden_layer_sizes': [(100,), (50, 50)],\n",
        "            'model__alpha': [0.0001, 0.001]\n",
        "        }\n",
        "    },\n",
        "    'KNN': {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'model__n_neighbors': [3, 5, 7],\n",
        "            'model__weights': ['uniform', 'distance']\n",
        "        }\n",
        "    },\n",
        "    'DecisionTree': {\n",
        "        'model': DecisionTreeClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'model__max_depth': [None, 5, 10],\n",
        "            'model__criterion': ['gini', 'entropy']\n",
        "        }\n",
        "    },\n",
        "    'ExtraTrees': {\n",
        "        'model': ExtraTreesClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'model__n_estimators': [100, 200],\n",
        "            'model__max_depth': [None, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    'GradientBoosting': {\n",
        "        'model': GradientBoostingClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'model__n_estimators': [100, 200],\n",
        "            'model__learning_rate': [0.05, 0.1],\n",
        "            'model__max_depth': [3, 5]\n",
        "        }\n",
        "    },\n",
        "    'Voting': {\n",
        "        'model': VotingClassifier(\n",
        "            estimators=[\n",
        "                ('svc', SVC(probability=True, random_state=42)),\n",
        "                ('rf', RandomForestClassifier(random_state=42)),\n",
        "                ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)),\n",
        "                ('mlp', MLPClassifier(max_iter=500, random_state=42))\n",
        "            ],\n",
        "            voting='soft'\n",
        "        ),\n",
        "        'params': {}  # No hyperparameters to tune here directly\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JLUrXiaovYCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the data"
      ],
      "metadata": {
        "id": "ycvMotRhekBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "erdWXRUCexri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loop on all the models to find the best parameters with a cross validation"
      ],
      "metadata": {
        "id": "uI1K6DThe3Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "best_models = {}\n",
        "\n",
        "for name, mp in model_params.items():\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessing', preprocessor),\n",
        "        ('model', mp['model'])\n",
        "    ])\n",
        "\n",
        "    clf = GridSearchCV(pipe, mp['params'], cv=5, n_jobs=-1, scoring='accuracy')\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters for {name}: {clf.best_params_}\")\n",
        "    print(f\"Validation Accuracy for {name}: {clf.best_score_:.4f}\")\n",
        "\n",
        "    best_models[name] = clf.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFMj_Ce8n3AX",
        "outputId": "e826c0db-593b-45e8-ba24-ff1c8fd52387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for SVC: {'model__C': 10, 'model__kernel': 'rbf'}\n",
            "Validation Accuracy for SVC: 0.8298\n",
            "Best parameters for RandomForest: {'model__max_depth': 10, 'model__n_estimators': 200}\n",
            "Validation Accuracy for RandomForest: 0.8368\n",
            "Best parameters for XGBoost: {'model__learning_rate': 0.05, 'model__max_depth': 5, 'model__n_estimators': 100}\n",
            "Validation Accuracy for XGBoost: 0.8368\n",
            "Best parameters for MLP: {'model__alpha': 0.001, 'model__hidden_layer_sizes': (100,)}\n",
            "Validation Accuracy for MLP: 0.8340\n",
            "Best parameters for KNN: {'model__n_neighbors': 3, 'model__weights': 'uniform'}\n",
            "Validation Accuracy for KNN: 0.8298\n",
            "Best parameters for DecisionTree: {'model__criterion': 'entropy', 'model__max_depth': 5}\n",
            "Validation Accuracy for DecisionTree: 0.8144\n",
            "Best parameters for ExtraTrees: {'model__max_depth': 10, 'model__n_estimators': 100}\n",
            "Validation Accuracy for ExtraTrees: 0.8354\n",
            "Best parameters for GradientBoosting: {'model__learning_rate': 0.05, 'model__max_depth': 5, 'model__n_estimators': 100}\n",
            "Validation Accuracy for GradientBoosting: 0.8242\n",
            "Best parameters for Voting: {}\n",
            "Validation Accuracy for Voting: 0.8425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display accuracy with the best parameters for each model"
      ],
      "metadata": {
        "id": "NIYwQkV4fU5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Optionally test on hold-out set\n",
        "for name, model in best_models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Test accuracy for {name}: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zdp56STfBfb",
        "outputId": "fa3848a8-879b-49af-c908-cead4db30124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for SVC: 0.7921\n",
            "Test accuracy for RandomForest: 0.7978\n",
            "Test accuracy for XGBoost: 0.7921\n",
            "Test accuracy for MLP: 0.8034\n",
            "Test accuracy for KNN: 0.7640\n",
            "Test accuracy for DecisionTree: 0.7865\n",
            "Test accuracy for ExtraTrees: 0.7978\n",
            "Test accuracy for GradientBoosting: 0.7978\n",
            "Test accuracy for Voting: 0.8202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Baseline Model Review\n",
        "\n",
        "Before introducing any feature engineering, we tested a variety of models using only the original features (`Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Fare`, and `Embarked`). Each model was tuned using `GridSearchCV` and evaluated on a hold-out test set.\n",
        "\n",
        "### 🔍 Performance Summary\n",
        "\n",
        "| Model              | Test Accuracy |\n",
        "|-------------------|----------------|\n",
        "| SVC               | 0.7921         |\n",
        "| RandomForest      | 0.7978         |\n",
        "| XGBoost           | 0.7921         |\n",
        "| MLP               | 0.8034         |\n",
        "| KNN               | 0.7640         |\n",
        "| DecisionTree      | 0.7865         |\n",
        "| ExtraTrees        | 0.7978         |\n",
        "| GradientBoosting  | 0.7978         |\n",
        "| **VotingClassifier** | **0.8202**     |\n",
        "\n",
        "- ✅ The **VotingClassifier** with soft voting performed best, reaching **82.02% accuracy**.\n",
        "- 🔁 However, this is close to a performance ceiling for the given features.\n",
        "- 🧠 To improve further, we now explore **feature engineering** to extract more signal from the data.\n",
        "\n",
        "---\n",
        "\n",
        "### ⏭️ Next: Feature Engineering\n",
        "\n",
        "We'll now enrich the dataset with new features like:\n",
        "- Titles extracted from passenger names\n",
        "- Family size & solo travelers\n",
        "- Cabin deck categories\n",
        "- Ticket groupings or fare bins\n",
        "\n",
        "These can help our models capture deeper patterns in the data.\n"
      ],
      "metadata": {
        "id": "u-CJ_oJlhHuv"
      }
    }
  ]
}